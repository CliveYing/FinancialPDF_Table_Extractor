{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Extration_Table.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"BarPOGGJeXKD","colab_type":"text"},"source":["#Installation part\n","\n","Please run this code snippet to install all the required packages"]},{"cell_type":"code","metadata":{"id":"27HOArT2OKnY","colab_type":"code","colab":{}},"source":["# We need to install a number of libraries and environment before we start\n","!apt-get install poppler-utils\n","!pip install pdf2image\n","\n","# install dependencies: (use cu100 because colab is on CUDA 10.0)\n","!pip install -U torch==1.4+cu100 torchvision==0.5+cu100 -f https://download.pytorch.org/whl/torch_stable.html \n","!pip install cython pyyaml==5.1\n","!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n","import torch, torchvision\n","torch.__version__\n","!gcc --version\n","# opencv is pre-installed on colab\n","\n","# install detectron2:\n","!pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu100/index.html\n","\n","# install pytersseract\n","! apt install tesseract-ocr\n","! apt install libtesseract-dev\n","!pip install pytesseract\n","!pip install tox\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCokEbaDovw-","colab_type":"text"},"source":["# Major Functions Snippet\n","\n","Run this code snippet to enable extraction and detection function."]},{"cell_type":"code","metadata":{"id":"3aR2iYfS1LKz","colab_type":"code","colab":{}},"source":["import os, fnmatch\n","from pdf2image import convert_from_path\n","from pdf2image.generators import (\n","    ThreadSafeGenerator,\n","    counter_generator)\n","\n","from pdf2image.exceptions import (\n","    PDFInfoNotInstalledError,\n","    PDFPageCountError,\n","    PDFSyntaxError)\n","\n","# You may need to restart your runtime prior to this, to let your installation take effect\n","# Some basic setup:\n","# Setup detectron2 logger\n","import detectron2\n","from detectron2.utils.logger import setup_logger\n","setup_logger()\n","\n","# import some common libraries\n","import pandas as pd\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","import numpy as np\n","from numpy import array\n","from numpy import argmax\n","from numpy import log\n","import cv2\n","import random\n","import math\n","from google.colab.patches import cv2_imshow\n","import json\n","from detectron2.structures import BoxMode\n","\n","\n","# import some common detectron2 utilities\n","from detectron2 import model_zoo\n","from detectron2.engine import DefaultPredictor\n","from detectron2.config import get_cfg\n","from detectron2.utils.visualizer import Visualizer\n","from detectron2.data.datasets import register_coco_instances\n","from detectron2.data import DatasetCatalog, MetadataCatalog\n","\n","from detectron2.data.datasets import register_coco_instances\n","from detectron2.data import DatasetCatalog, MetadataCatalog\n","from detectron2.utils.visualizer import ColorMode\n","\n","# Pytesseract\n","try:\n","    from PIL import Image\n","except ImportError:\n","    import Image\n","import pytesseract\n","import tox\n","\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Embedding, LSTM, Dense, Dropout\n","from keras.preprocessing.text import Tokenizer\n","from keras.callbacks import EarlyStopping\n","from keras.models import Sequential\n","import keras.utils as ku \n","import matplotlib.pyplot as plt\n","\n","from google.colab import files\n","from google.colab import drive\n","\n","import sqlite3\n","\n","conn = sqlite3.connect('Table_DB.db')\n","\n","def convert_pdf_to_jpg(input_path,output_path):\n","    # create the folder to store all images from multiple pdfs\n","    if not os.path.exists(output_path): \n","        os.makedirs(output_path, exist_ok=True)\n","\n","    # checks if path is a file or a directory\n","    # pdfNames is the output, list of all pdf files' path\n","    pdfNames = []\n","    if os.path.isfile(input_path):\n","        print('The input is one pdf file.')\n","        pdfNames = [input_path]\n","    else:\n","        print('The input is multiple pdf files.')\n","        listOfFiles = os.listdir(input_path)\n","        pattern = \"*.pdf\"\n","\n","        # build the list for all file names\n","        for entry in listOfFiles:\n","            if fnmatch.fnmatch(entry, pattern):\n","                pdfNames.append(input_path + '/'+ entry)\n","        pdfNames = sorted(pdfNames)\n","\n","    for inputpdf in pdfNames:\n","        print(\"Transfering to JPG for table detection:\\n\",inputpdf,'\\nThis may take a while...')\n","\n","        # create the folder to store all images from one pdf\n","        image_folder = output_path + '/' + inputpdf.split('/')[-1] + '_dir'\n","        if not os.path.exists(image_folder): \n","            os.makedirs(image_folder, exist_ok=True)\n","\n","        # convert pdf pages into images automatically\n","        # pdfName.pdf.1-pageNumber(3 digits).jpg\n","        generator = convert_from_path(inputpdf, dpi=200, fmt = 'jpeg',\n","                                        output_folder = image_folder,\n","                                        output_file = counter_generator(inputpdf.split('/')[-1] + '.', padding_goal=0))\n","        \n","        # pageNumber_pdfName.pdf.jpg\n","        for count, filename in enumerate(os.listdir(image_folder)):\n","            num_page = int(filename.split('1-',1)[1].split('.jpg')[0])\n","            dst = str('{0:04d}'.format(num_page)) + '_' + filename.split('1-',1)[0] +  \"jpg\"\n","            src = image_folder + '/' + filename \n","            dst = image_folder + '/' + dst \n","\n","            # rename all the files \n","            os.rename(src, dst)\n","    print('Convert all PDF files succussfully!')\n","    return pdfNames\n","\n","# loading the detection model\n","def load_detection_model(training_data_json, training_data, final_detection_model):\n","\n","    ## model should be put in the same workspace.\n","    cfg = get_cfg()\n","    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n","    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_X_101_32x8d_FPN_3x.yaml\"))\n","    register_coco_instances(\"my_dataset_train\", {}, training_data_json, training_data)\n","\n","    cfg.SOLVER.IMS_PER_BATCH = 2\n","    cfg.SOLVER.BASE_LR = 0.001\n","    cfg.SOLVER.MAX_ITER = 450 \n","    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  \n","    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (table)\n","\n","    ## We can change the model to be used\n","    ## Detection model address may need to be changed\n","    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, final_detection_model)\n","    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.95  # set the testing threshold for this model\n","\n","    predictor = DefaultPredictor(cfg)\n","    table_train_metadata = MetadataCatalog.get(\"my_dataset_train\")\n","\n","    return predictor, table_train_metadata\n","\n","\n","# \n","def matt_filter(crop):\n","  custom_oem_psm_config = r'--oem 3 --psm 6'\n","  sl = pytesseract.image_to_data(crop, lang = 'eng',\n","                                          output_type = pytesseract.Output.DATAFRAME, \n","                                          config = custom_oem_psm_config)\n","  if firstFilter(sl):\n","    return True\n","  else:\n","    return False\n","\n","def firstFilter(pdFrame):\n","  counter = 0\n","  targetList = [\"class\",\"Pass-Through\", \"S&P\",\"Fitch\", \"Moody's\", \"LIBOR Rate\", \"Reallowance Discount\"]\n","  for i in pdFrame.iloc[:,11]:\n","    if(type(i) != float):\n","      for target in targetList:\n","        if(target.casefold() in i.casefold()):\n","          targetList.remove(target)\n","          counter = counter +1\n","          if (counter >= 2):\n","            print('matt_true')\n","            return True\n","\n","  return False\n","\n","class TableHero(object):\n","    def __init__(\n","        self,\n","        input_path           = None,\n","        pdfNames             = [],\n","        jpgFileName          = '/content/allPage_Image',\n","        # change the path of json file of training data\n","        training_data_json   = '/content/drive/My Drive/unit of study/capstone/dataset/user_version/training_data.json',\n","        # change the path of training data\n","        training_data        = '/content/drive/My Drive/unit of study/capstone/dataset/user_version/training_data',\n","        # change the path of detection model\n","        final_detection_model= '/content/drive/My Drive/unit of study/capstone/model/model_final_15_450.pth',\n","        predictor            = None,\n","        table_train_metadata = None,\n","        crop_table_list      = None,\n","    ):\n","\n","        self.input_path           = input_path\n","        self.jpgFileName          = jpgFileName\n","        self.pdfNames             = convert_pdf_to_jpg(input_path,jpgFileName)\n","        \n","        self.training_data_json   = training_data_json\n","        self.training_data        = training_data\n","        self.final_detection_model= final_detection_model\n","        self.predictor, self.table_train_metadata = load_detection_model(training_data_json,\n","                                                                         training_data,\n","                                                                         final_detection_model)\n","\n","    def detect_tables(self):\n","        ## Matthew: This is the path for img with bounding box of tables\n","        if not os.path.exists('Page_with_Bbox'): \n","            os.makedirs('Page_with_Bbox', exist_ok=True)\n","\n","        ## Matthew: This is the path for cropped table img\n","        if not os.path.exists('Table_JPG'): \n","            os.makedirs('Table_JPG', exist_ok=True)\n","        \n","        table_info = []\n","        id = 0\n","        print(len(self.pdfNames))\n","        for file_name in self.pdfNames:\n","            # create folder for each pdf file\n","            if not os.path.exists('/content/Page_with_Bbox/' + file_name.split('/')[-1] + '_dir'): \n","                os.makedirs('/content/Page_with_Bbox/' + file_name.split('/')[-1] + '_dir', exist_ok=True)\n","\n","            # read each pdf file and its' list of all page images\n","            print('Detecting pdf: ', file_name)\n","            jpg_name_list = os.listdir(self.jpgFileName + '/' + file_name.split('/')[-1] + '_dir')\n","            jpg_name_list.sort()\n","\n","            # detect tables in each page image\n","            for image in jpg_name_list:\n","                page = image.split('_')[0]\n","                # d is the path of each page image\n","                d = self.jpgFileName + '/' + file_name.split('/')[-1] + '_dir/' + image\n","                \n","                im = cv2.imread(d)\n","                outputs = self.predictor(im)\n","                table_coordinates = outputs[\"instances\"].pred_boxes.tensor.tolist()\n","\n","                v = Visualizer(im[:, :, ::-1],\n","                                metadata= self.table_train_metadata, \n","                                scale=0.5, \n","                                )\n","\n","                # check if table exists\n","                if not table_coordinates:\n","                    a = 'no page'\n","                    # print(\"There is no table in this page: \", page)\n","                else:\n","                    print(\"In page: \" + str(page), ', There are ' + str(len(table_coordinates)) + ' tables.')\n","                    print(\"Table edge coordinates are: \", table_coordinates)\n","\n","                    # Drawing the page with tables\n","                    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n","                    page_with_bbox = v.get_image()[:, :, ::-1]\n","                    # cv2_imshow(page_with_bbox)\n","\n","                    # pwbName = str(page) + '_' + str(len(table_coordinates)) + '_' + image.split('_',1)[1]\n","                    # save image, page with bounding boxes, in the folder\n","                    if not cv2.imwrite(os.path.join('/content/Page_with_Bbox/' + file_name.split('/')[-1] + '_dir', image), page_with_bbox):\n","                        raise Exception(\"Could not write image\")\n","                    \n","                    \n","                    # loop tables in one page\n","                    for i in range(len(table_coordinates)):\n","                        table_imageName = str(page) + '_' + str(i+1) + '_' + image.split('_',1)[1]\n","                        x0 = int(table_coordinates[i][0])\n","                        y0 = int(table_coordinates[i][1])\n","                        x1 = int(table_coordinates[i][2])\n","                        y1 = int(table_coordinates[i][3])\n","                        crop = im[y0:y1, x0:x1]\n","\n","                        id += 1\n","                        pdfFileName = file_name.split('/')[-1]\n","                        pageNum  = str(page)\n","                        tableNum = str(i+1)\n","                        pageWithBbox = image\n","\n","                        # save image, table under bounding box, in the folder\n","                        if matt_filter(crop):\n","                            cv2_imshow(crop)\n","                            tableJPG = table_imageName\n","                            if not cv2.imwrite(os.path.join('/content/Table_JPG', table_imageName), crop):\n","                                raise Exception(\"Could not write image\")\n","                        else:   \n","                            tableJPG = ''\n","                        table_info.append((id, pdfFileName, pageNum, tableNum, pageWithBbox, tableJPG))\n","                            \n","        cropfileName = '/content/Table_JPG/'\n","        self.crop_table_list = os.listdir(cropfileName)\n","        self.crop_table_list.sort()\n","\n","        header = ['id', 'PDF File Name', 'Page Number', 'Table Number', 'Page With Bounding Box', 'Table Image Name']\n","        table_info_df = pd.DataFrame.from_records(table_info, columns = header)\n","        print(table_info_df)\n","\n","        # save detection result into csv\n","        table_info_df.to_csv('/content/detectionResult.csv')\n","\n","\n","\"\"\"# Convert image into dataframe\n","\n","### Func: image_to_data\n","1. image with title: par_num:0,1 block_num = 1, texts are title； \n","2. image without title: part_num:0,1,2 header order by columns' location\n","\"\"\"\n","\n","# Function to convet jpg to dataframe\n","def to_data_psm6(image_path):\n","    # psm 6 = Assume a single uniform block of text.\n","    custom_oem_psm_config6 = r'--oem 3 --psm 6'\n","    image_psm6 = pytesseract.image_to_data(image_path, lang = 'eng',\n","                                                output_type = pytesseract.Output.DATAFRAME, \n","                                                config = custom_oem_psm_config6)\n","    return image_psm6\n","\n","\"\"\"### cutting title in table\"\"\"\n","\n","# Function to decide whether the tile exists\n","def if_title(image_psm6):\n","    # par_num = 2 means the body\n","    if 2 in image_psm6.get('par_num').array:\n","        return True\n","    else:\n","        return False\n","\n","# Function to cut the tile, based on if_title()\n","def cut_title(image,image_psm6,image_path):\n","    \n","    # image -> x0:left,y0:top,x1:right,y1:below\n","    x0 = 0 \n","    y0 = 0\n","    x1 = image.shape[1]\n","    y1 = int(image.shape[0])\n","\n","    # fing y0, new top coordinate below title\n","    for i in image_psm6.values:\n","        line   = i[4]\n","        top    = i[7]\n","        height = i[9]\n","        text   = i[-1]\n","\n","        # generaly, title is in one line\n","        if line == 1:\n","            if str(text) != 'nan' and str(text) != ' ':\n","                print(text)\n","            y0 = int(top + height)\n","        elif line == 2:\n","            # break in second line\n","            break\n","\n","    # croping the image under title\n","    crop = image[y0+10:y1, x0:x1][:, :, ::-1]\n","    cv2_imshow(crop) \n","    \n","    # replace the image\n","    if not cv2.imwrite(os.path.join(image_path), crop):\n","                    raise Exception(\"Could not replace the image\")\n","    else: print('The title was deleted successfully.')\n","\n","# Function to save image after cutting the tile, based on cut_title()\n","def update_image(image_path):\n","    # read image\n","    image = cv2.imread(image_path)\n","\n","    # psm 6\n","    image_psm6 = to_data_psm6(image_path)\n","    # print(image_psm6)\n","\n","    # decide if there exists title in table.\n","    # if so, cut title\n","    if if_title(image_psm6) is False:\n","        print('The table '+ image_path + ' may exist title.')\n","        cut_title(image,image_psm6,image_path)\n","\n","    # if not, continue\n","    else:\n","        print('The table '+ image_path + ' has no title.')\n","    \n","    return image_path\n","\n","\"\"\"## image to row list \n","combine texts from each cell in one row\n","\"\"\"\n","\n","# Function to extract info in one row\n","def extract_each_row_info(image_psm6):\n","    # set a new dictionary to store each text info in each row\n","    each_row_info = {}\n","\n","    # df by psm 6\n","    im_value = image_psm6.values\n","    for i in range(len(im_value)):\n","        # info in each cell\n","        line   = im_value[i][4]\n","        word   = im_value[i][5]\n","        left   = im_value[i][6]\n","        top    = im_value[i][7]\n","        width  = im_value[i][8]\n","        height = im_value[i][9]\n","        text   = im_value[i][11]\n","        info   = [line, left, top, width, height, text]\n","\n","        # number of rows\n","        num_row = len(each_row_info)\n","\n","        # change\n","        # change\n","        # change\n","        # change\n","        if str(text) != 'nan' \\\n","            and str(text) != ' ' \\\n","            and str(text) != '—'\\\n","            and str(text) != '_'\\\n","            and str(text) != '__':\n","\n","            # add the text in the same row\n","            # 'same row' means same line and top coordinate, and strarting word > 1\n","            if num_row > 0 \\\n","                    and line == each_row_info[num_row-1][0][0] \\\n","                    and word > 1 \\\n","                    and top - each_row_info[num_row-1][0][2] < 50:\n","                    # print(each_row_info[num_row-1][word-2])\n","                    if each_row_info[num_row-1][-1][5] == '$':\n","                        info[5] = '$'+ text\n","                        each_row_info[num_row-1][-1] = info\n","                    else:\n","                        # print(type(text),text)\n","                        each_row_info[num_row-1].append(info)\n","                        # print(each_row_info[num_row-1])\n","            \n","            else:\n","                # sort info in each row by left coordinates\n","                if num_row > 0:\n","                    each_row_info[num_row-1].sort(key=lambda x: x[1])\n","\n","                # create a new row\n","                num_row = len(each_row_info)\n","                each_row_info[num_row] = [info]\n","                # print(each_row_info[num_row],num_row)\n","\n","    return each_row_info\n","\n","# Function to keep left coordinate and text in each cell in one row\n","def text_rc(each_row_column):\n","    # print(each_row_column)\n","    each_cell = []\n","    for i in range(len(each_row_column)):\n","        # info in list\n","        left   = each_row_column[i][0]\n","        width  = each_row_column[i][1]\n","        center = left+width/2\n","        text   = each_row_column[i][3]\n","\n","        if i > 0:\n","            last_center = each_cell[0]\n","            last_text = each_cell[1]\n","\n","            # keep the leftest coordinate\n","            if center < last_center:\n","                each_cell[0] = center\n","            # integrate the text in one cell\n","            each_cell[1] = last_text + ' ' + text\n","\n","        else:\n","            each_cell.append(center)\n","            each_cell.append(text)\n","    return each_cell\n","\n","# Function to integrate texts in each row\n","def combine_info(row_list):\n","    # create a new list to store the text in each row\n","    each_row = []\n","    length = len(each_row)\n","    for i in range(len(row_list)):\n","        # info without combination\n","        line   = row_list[i][0]\n","        left   = row_list[i][1]\n","        top    = row_list[i][2]\n","        width  = row_list[i][3]\n","        height = row_list[i][4]\n","        text   = row_list[i][5]\n","        \n","        # x_center = int(left + width/2)\n","        # y_center = int(left + width/2)\n","\n","\n","        # combine text if the space is too short\n","        if i > 0:\n","            # last info after combination \n","            last_left  = each_row[length-1][-1][0]\n","            last_width = each_row[length-1][-1][1]\n","            last_top   = each_row[length-1][-1][2]\n","\n","            # horizontal space between two text\n","            h_space = left - last_left - last_width\n","            # vertical space between two text\n","            v_space = top - last_top\n","            # print(text,each_row[length-1][-1][-1],h_space,v_space)\n","\n","            \n","            if h_space < 26 and v_space < -10:\n","                each_row[length-1].insert(-1,[left,width,top,text])\n","            elif h_space < 26 and v_space >= -10:\n","                each_row[length-1].append([left,width,top,text])\n","            else:\n","                each_row[length-1] = text_rc(each_row[length-1])\n","                each_row.append([[left,width,top,text]])\n","        else:\n","            each_row.append([[left,width,top,text]])\n","\n","    each_row[-1] = text_rc(each_row[-1])\n","    return each_row\n","\n","# Function to output each row\n","def output_row(each_row_info):\n","    row_dic = []\n","    for k in range(len(each_row_info.values())):\n","        row_list = each_row_info[k]\n","        each_row = combine_info(row_list)\n","        row_dic.append(each_row)\n","    return row_dic\n","\n","\"\"\"## row list to dataframe\"\"\"\n","\n","from bisect import bisect_left\n","\n","def take_closest(myList, myNumber):\n","    \"\"\"\n","    Assumes myList is sorted. Returns closest value to myNumber.\n","\n","    If two numbers are equally close, return the smallest number.\n","    \"\"\"\n","    pos = bisect_left(myList, myNumber)\n","    if pos == 0:\n","        return myList[0]\n","    if pos == len(myList):\n","        return myList[-1]\n","    before = myList[pos - 1]\n","    after = myList[pos]\n","    if after - myNumber < myNumber - before:\n","       return after\n","    else:\n","       return before\n","\n","def structure_body(row_dic):\n","    # max number of cells in one row\n","    max_len = 0\n","    max_index = None\n","    for idx, cell in enumerate(row_dic):\n","        if len(cell) >= max_len:\n","            max_len   = len(cell)\n","            max_index = idx\n","\n","    # list of center coordinates in longest row\n","    center_list = [x[0] for x in row_dic[max_index]]\n","    # print(center_list)\n","    # print(max_len, max_index)\n","\n","    for idx,cell in enumerate(row_dic):\n","        for i in cell:\n","            i[0] = take_closest(center_list,i[0])\n","\n","    for key, cell in enumerate(row_dic):\n","        # print(cell)\n","        cell_in_row = {}\n","        for idx in range(max_len):\n","            center = center_list[idx]\n","            # print(center)\n","            cell_in_row[idx] = np.nan\n","            for i in cell:\n","                # print(i)\n","                if center-100 <= i[0] <= center+100:\n","                    cell_in_row[idx] = str(i[1])\n","                    break\n","            # print(cell_in_row[idx])\n","\n","        row_dic[key] = cell_in_row\n","\n","    first_r = [x for x in row_dic[0].values() if str(x) != 'nan']\n","    if len(first_r) <= 1 and len(first_r[0].split(' ')) > 4:\n","        del row_dic[0]\n","\n","    return row_dic\n","        # break\n","\n","\"\"\"# Separate header and body\n","\n","## Beam-search\n","\"\"\"\n","\n","# Commented out IPython magic to ensure Python compatibility.\n","## this is a naive dataset for typical financial table headings \n","data = '''Mortgage Loan\n","Cut-off Date Principal Balance\n","# % ofInitial Outstanding Pool Balance\n","Lock- Out Period (months from Cut-off Date)\n","Year\n","Approximate Initial Principal Amount of Year Securities\n","Property Type\n","Number of Mortgage Loans\n","Aggregate Cut-off Date Principal Balance ($)\n","# % of Mortgage Pool by Aggregate Cut-off Date Principal Balance (4)\n","Weighted Average Gross Interest Rate (%)\n","Weighted Average Remaining Term (months)\n","Weighted Average Combined Original LTV (%)\n","Loan Purpose\n","Number of Mortgage Loans\n","Principal Balance\n","Principal Amount\n","Standard Interest Rate\n","Percentage of Mortgage Loans\n","Average Principal Balance\n","Weighted Average Credit Score\n","Weighted Average Loan-to-Value Ratio\n","Initial Certificate Principal Balance”\n","Class\n","Pass-Through Rate\n","Scheduled Final Maturity Date\n","Distribution Date\n","Offered Certificates\n","Stated Maturity\n","Initial Offering Price\n","S&P Rating\n","Moody's Rating\n","'''\n","tokenizer = Tokenizer()\n","max_sequence_len = None\n","\n","## model functions\n","def dataset_preparation(data):\n","\n","    # basic cleanup\n","    corpus = data.lower().split(\"\\n\")\n","\n","    # tokenization\t\n","    tokenizer.fit_on_texts(corpus)\n","    total_words = len(tokenizer.word_index) + 1\n","\n","    # create input sequences using list of tokens\n","    input_sequences = []\n","    for line in corpus:\n","        token_list = tokenizer.texts_to_sequences([line])[0]\n","        for i in range(1, len(token_list)):\n","            n_gram_sequence = token_list[:i+1]\n","            input_sequences.append(n_gram_sequence)\n","\n","    # pad sequences \n","    max_sequence_len = max([len(x) for x in input_sequences])\n","    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","    # create predictors and label\n","    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","    # convert to one-hot \n","    label = ku.to_categorical(label, num_classes=total_words)\n","\n","    return predictors, label, max_sequence_len, total_words\n","\n","def create_model(predictors, label, max_sequence_len, total_words):\n","    # LSTM model\n","    model = Sequential()\n","    model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))\n","    model.add(LSTM(150, return_sequences = True))\n","\n","    model.add(LSTM(100))\n","    model.add(Dense(total_words, activation='softmax'))\n","\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","    earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n","\n","    model.fit(predictors, label, epochs=20, verbose=1, callbacks=[earlystop])\n","\n","    print(model.summary())\n","    return model \n","\n","## beam-search functions\n","# convert index to word\n","\n","\n","def ind_to_word(predicted_ind):\n","  answer = ''\n","  for i in predicted_ind:\n","    for word, index in tokenizer.word_index.items():\n","        if index == i:\n","          answer = answer+word+' '\n","  return answer    \n","\n","\n","# get the top k most predicted results\n","###\n","\n","def get_topK(predicted, k):\n","    \n","    top_k = np.argsort(predicted[0])[-k:]\n","\n","    return top_k, predicted[0][top_k]\n","\n","\n","# generate text, currently only works with k=1 \n","# (does not store candidates, you need to modify the code to store candidates and pick the highest scored sequence)\n","\n","def generate_text(seed_text, initial_candidate_number, max_sequence_len, k=1):\n","  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","  token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","  predicted = model.predict(token_list)\n","  \n","  first_round_candidates, first_probs = get_topK(predicted,initial_candidate_number)\n","  \n","  # First set of candidates based on seed text\n","  candidates = []\n","  for i in range(initial_candidate_number):\n","    candidates.append([[first_round_candidates[i]],first_probs[i]])\n","  initial_candidates = []\n","  for kkk in range(initial_candidate_number):\n","    initial_candidates.append(ind_to_word(candidates[kkk][0]))\n","\n","  # Using beamserch method to get top candidates\n","  candidates = beam_next(candidates, seed_text, k)\n","  best_sequence = candidates[:1]\n","  output_word = ind_to_word(best_sequence[0][0])\n","        \n","  seed_text += \" \" + output_word\n","        \n","  return seed_text, candidates, candidates_to_text(candidates)\n","\n","def candidates_to_text(candidates):\n","  text_list = []\n","  for c in candidates:\n","    text_list.append(ind_to_word(c[0]))\n","  return text_list\n","\n","# you can add more function if you want to\n","def beam_next(candidates, seed_text, k):\n","  K=k-1\n","\n","  for kk in range(K):\n","\n","    for i in range(k):\n","      temp_text = seed_text + \" \" + ind_to_word(candidates[i][0])\n","      token_list = tokenizer.texts_to_sequences([temp_text])[0]\n","      token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","      predicted = model.predict(token_list)\n","      round_candidates, probs = get_topK(predicted,k)\n","      for x in range(k):\n","        new_candidate = []\n","        for c in candidates[i][0]:\n","          new_candidate.append(c)\n","        new_candidate.append(round_candidates[x])\n","        new_prob = candidates[x][1] + probs[x]\n","        candidates.append([new_candidate,new_prob])\n","        \n","      \n","    ordered = sorted(candidates, key=lambda tup:tup[1], reverse=True)\n","    candidates = ordered[:k]\n","   \n","  return candidates\n","\n","# Train a naive s2s model for table headers\n","predictors, label, max_sequence_len, total_words = dataset_preparation(data)\n","model = create_model(predictors, label, max_sequence_len, total_words)\n","\n","def s2s_beamsearch_extract(table): \n","  # table = table.df\n","  combinedRow = ()\n","  rowNo = len(table)\n","  columnNo = len(table.columns)\n","  savedList = []\n","  headerFrame = [\"\"] * columnNo\n","  titleList = [\"\"] * columnNo\n","  previous_guest_list = [[]] * columnNo\n","  pointToBreak = 0\n","  listToAdd =[]\n","  booleanList = [False] * columnNo\n","  skip = False\n","  previousRowIsTitle = False\n","\n","  # First for loop should loop through the whole table, and if statement should \n","  # used here to determine when we should stop with the first for loop\n","  # Row loop\n","  for i in range(0,rowNo):\n","    if(i == pointToBreak & i != 0):\n","      break\n","    else:\n","     \n","      tempList = previous_guest_list\n","      tupleToAdd = ()\n","      guessList = [[]] * columnNo\n","\n","      #Column Loop\n","      for j in range(0,columnNo): \n","        if (i==0):\n","          if (str(table[j][i])==\"\"):\n","            guessList[j] = []\n","            headerFrame[j] = ''\n","            titleList[j] = False\n","          \n","          \n","          else:\n","            targetString = str(str(table[j][i]))\n","            targetStringLower = targetString.lower()    \n","            top_result, candidates_ind_prob, candidates_result = generate_text(targetStringLower, 10, max_sequence_len, k=1)\n","            guessList[j] = candidates_result\n","            headerFrame[j] = headerFrame[j] + targetString\n","            titleList[j] = True\n","\n","        else:\n","          if (skip == True):\n","            skip = False\n","            headerFrame = [\"\"] * columnNo\n","            previous_guest_list = [[]] * columnNo\n","            if (str(table[j][i])==\"\"):\n","              guessList[j] = []\n","              headerFrame[j] = ''\n","              \n","            else:\n","              targetString = str(table[j][i])\n","              targetStringLower = targetString.lower()\n","              headerFrame[j] = headerFrame[j] + targetString\n","\n","\n","          else:\n","            if (str(table[j][i])==\"\"):\n","              if(headerFrame[j] ==\"\"):\n","    \n","                guessList[j] = []\n","                headerFrame[j] = ''\n","              else:\n","                booleanList[j] = False\n","              \n","            else:\n","            \n","              targetString = str(str(table[j][i]))\n","              targetStringLower = targetString.lower()\n","              \n","              if not tempList[j]:\n","                top_result, candidates_ind_prob, candidates_result = generate_text(targetString, 10, max_sequence_len, k=1)\n","                guessList[j] = candidates_result\n","                headerFrame[j] = headerFrame[j] + targetString\n","              else:\n","                for item in tempList[j]:\n","                  ##### This part has to be reviewed #######\n","                  if(item.strip() in targetStringLower):\n","                    headerFrame[j] = headerFrame[j] +\" \" + targetString\n","                    booleanList[j] = True\n","                    top_result, candidates_ind_prob, candidates_result = generate_text(headerFrame[j], 10, max_sequence_len, k=1)\n","                    guessList[j] = candidates_result\n","                    break\n","                  else:\n","                    booleanList[j] = False\n","\n","        previous_guest_list= guessList\n","\n","      if (i != 0):\n","        if any(booleanList) :\n","          print(\" \")\n","        else:\n","          if (previousRowIsTitle != True):\n","            pointToBreak = i+1\n","          else:\n","            previousRowIsTitle = False\n","    \n","    if(i == 0):\n","      \n","      for count in range(len(titleList)):\n","       \n","        number = math.trunc(columnNo/2) - 1\n","        if (titleList[count] == True and count == number and sum(titleList) == 1):\n","          headerFrame = [\"\"] * columnNo\n","          previous_guest_list = [[]] * columnNo\n","          skip = True\n","          previousRowIsTitle = True\n","          # booleanList[columnNo-1] = True\n","        else:\n","          print(\"  \")\n","    else:\n","      skip = False\n","    \n","\n","\n","  contentStartRow = pointToBreak -1\n","  for i in range(contentStartRow, rowNo):\n"," \n","    tupleToAdd = ()\n","    cellFrame = [\"\"] * columnNo\n","    for j in range(0, columnNo):\n","      try:\n","        cellFrame[j] = cellFrame[j] +\" \"+ str(table[j][i])\n","      except:\n","        continue\n","    tupleToAdd = tuple(cellFrame)\n","    savedList.append(tupleToAdd)\n","\n","  result = pd.DataFrame(savedList , columns = headerFrame)\n","  return result\n","\n","\"\"\"## regular\"\"\"\n","\n","def is_a_header_row(s):\n","    \n","    keyword_list = ['class','%','$','billion',\n","                    'November','A-','February',\n","                    'II','M-','I-A-A','I-A-B',\n","                    'II-A-','A-A','A-B',\n","                    'Ifl-A-','September','+','AAA','Class',\n","                   '.%',',',',,',',,,']\n","    \n","    header_keyword_list = ['Date','Balance','Rate','S&P','Moody','Certificates']\n","    \n","    s_no_number = ''.join([i for i in s if not i.isdigit()])\n","    s_no_number = ''.join(s_no_number.split('\\n'))\n","    s_list = s_no_number.split(' ')\n","    hit = 0\n","    for item in s_list:\n","        if item in keyword_list:\n","            hit += 1\n","        else:\n","            for char in item:\n","                 if char in keyword_list:\n","                    hit+=1\n","\n","    for item in s_list:\n","        if item in header_keyword_list:\n","            hit = 0\n","\n","    if hit >= 2:\n","        return False\n","    else:\n","        return True\n","    \n","    \n","def regular_rows_identifier(df):\n","    row_array = df.text.values\n","    row_class = []\n","    for row in row_array:\n","        if is_a_header_row(row):\n","            row_class.append(0)\n","        else:\n","            row_class.append(1)\n","    return row_class\n","\n","\n","def table_row_to_text(table):\n","    text_from_table = ''\n","    total_text_list = []\n","    # for r_idx in range(len(table.rows)):\n","    for r_idx in range(table.shape[0]):\n","        text_from_table = ''\n","        # for c_idx in range(len(table.cols)):\n","        for c_idx in range(table.shape[1]):\n","            text_from_table = text_from_table + ' ' + str(table.values[r_idx][c_idx]).split('...')[0]\n","        total_text_list.append(text_from_table)\n","    return total_text_list\n","\n","def regular_expression_identifier(table):\n","    df = pd.DataFrame(table_row_to_text(table), columns=['text'])\n","    header_body_list = regular_rows_identifier(df)\n","    \n","    # count how many headers are identified\n","    header_index = 1\n","    while header_index < table.shape[0]:\n","        if header_body_list[header_index] == 0:\n","            header_index += 1\n","        else:\n","            break\n","   \n","    # making header array\n","    cols_number = table.shape[1]\n","    header_array = []\n","    for r_idx in range(header_index):\n","        row_header = []\n","        for c_idx in range(cols_number):\n","            row_header.append(str(table.values[r_idx][c_idx]).replace('\\n', ' ').replace('nan',''))\n","        \n","        if header_array:\n","            for cc in range(cols_number):\n","                header_array[cc] = header_array[cc] + ' ' + row_header[cc]\n","        else:\n","            header_array = row_header\n","        \n","    #making body arrays\n","    body_array = []\n","    while header_index < len(header_body_list):\n","        each_body_array = []\n","        for c_idx in range(cols_number):\n","            each_body_array.append(str(table.values[header_index][c_idx]).replace('\\n', ' ').replace('nan',''))\n","        body_array.append(each_body_array)\n","        header_index += 1\n","    \n","    reformed_df = pd.DataFrame(body_array, columns = header_array) \n","    \n","    return header_array, body_array, reformed_df\n","\n","\"\"\"# Read image and Save CSV\"\"\"\n","\n","def read_from_path(folder_path):\n","    listOfJPG = os.listdir(folder_path)\n","    \n","\n","    for index,img in enumerate(listOfJPG):\n","        filename = os.path.join(folder_path, img)\n","        listOfJPG[index] = filename\n","    return listOfJPG\n","\n","def extract_table(folder_path,method_name):\n","    listOfJPG = read_from_path(folder_path)\n","\n","    dataf = pd.DataFrame()\n","    # print(len(listOfJPG))\n","    for image_path in listOfJPG:\n","        image_path = image_path\n","        # foler name is the PDF file name, file name is the page number plus PDF name\n","        # folder_name = image_path.split('/')[-1].split('_')[1].split('.')[0]\n","        file_name   = image_path.split('/')[-1].split('.')[0]\n","\n","        if image_path.split('.')[-1] != 'jpg':\n","            print(image_path + 'is not the image.')\n","        print('image_path: --',image_path)\n","\n","        ## todo Tin, we can use update_image to cut titiles. However, the performance\n","        ## is not stable.\n","        # image_path = update_image(image_path)\n","\n","        # extract image TSV by py-tesseract\n","        image_psm6 = to_data_psm6(image_path)\n","        # print(image_psm6)\n","\n","\n","        each_row_info = extract_each_row_info(image_psm6)\n","        row_dic = structure_body(output_row(each_row_info))\n","        # for i in row_dic:\n","        #     print(i)\n","        df = pd.DataFrame(row_dic).dropna(how='all')\n","        # print(df)\n","\n","\n","        # combine header by bean search or regular\n","        if method_name == 'beamsearch':\n","            final_df = s2s_beamsearch_extract(df)\n","        elif method_name == 'regular':\n","            header,body,final_df = regular_expression_identifier(df)\n","        \n","        # create a folder to save csv file\n","        # if not os.path.exists('/content/csv'):\n","        #     os.makedirs('/content/csv', exist_ok=True)\n","\n","        # check \n","        if False in final_df.columns.duplicated():\n","            final_df = final_df.groupby(final_df.columns.values, axis=1).agg(lambda x: x.values.tolist()).sum().apply(pd.Series).T\n","        \n","        final_df.columns = list(i.strip() for i in final_df.columns)\n","\n","        # write into DB\n","        final_df.to_sql(file_name, conn, if_exists='replace', index=True)\n","\n","        # save dateframe into csv if you want\n","        final_df.to_csv('/content/csv/' + file_name + '.csv')\n","    \n","        # file name, page number, table number, columns\n","        page_num = int(file_name.split('_')[0])\n","        table_num = int(file_name.split('_')[1])\n","\n","        # get the column\n","        for col in final_df.columns:\n","            row = pd.DataFrame([[file_name, page_num, table_num, col]],columns=['file name', 'page number', 'table number', 'column'])\n","            dataf = pd.concat([dataf, row], ignore_index=True)\n","    \n","    # print(dataf)\n","    dataf.to_sql('ColumnSummary', conn, if_exists='replace', index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnUoPYvN2e-M","colab_type":"text"},"source":["# Demo \n"]},{"cell_type":"code","metadata":{"id":"PASWaHU6vF0x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1593684574216,"user_tz":-600,"elapsed":238368,"user":{"displayName":"Mengting Wu","photoUrl":"","userId":"08906690419009667508"}},"outputId":"14f6718c-3cd8-4de1-bccf-3d3dfa8af14c"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PJhpUxUJfR47","colab_type":"text"},"source":["### Input path of root folder\n","\n","This part here is to setup the directory path of your root folder\n","\n","The easiest way to get your file path that contains all of your pdf document is\n","to : \n","\n","1. First click the file button on the left navigation bar, you should able to \n","see there is a drive file\n","\n","2. Click it and there is a file name My Drive, after you\n","expand the \"My Drive\" folder, you would be able to see all the files that your google drive has\n","\n","3. Right click the folder that contains all of your pdf documents\n","that you want to detect/extract\n","\n","4. Click copy path, and paste the new root folder path into the root_folder params\n","\n","5. After everything is set, run this code snippet."]},{"cell_type":"code","metadata":{"id":"HOcVHMmCQRMT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1593139712179,"user_tz":-600,"elapsed":976,"user":{"displayName":"Matthew Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig2zs-xXencuoCLb7KGW9XqNONCd67o5omaxOP=s64","userId":"15620748170229253400"}},"outputId":"598f8ebe-22a8-41d5-9f00-9bc9ac569fe0"},"source":["root_folder = \"/content/drive/My Drive/capstone/Fianl slides/test files\" #@param {type:\"string\"}\n","listFiles = os.listdir(root_folder)\n","print(listFiles)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['20060131_!!00NE2G_Prospectus_SD000000000017379366.pdf', '20070619_!!00XY0D_Prospectus_SD000000000064855320.pdf', '20070705_!!00YPNU_Prospectus_SD000000000065504230.pdf', 'Statement.pdf']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j6Nxq7HBgfao","colab_type":"text"},"source":["### Select PDF files needed to be extracted\n","\n","In this part you are able to select single pdf as your target or all of the pdf\n","documents that exist in your root_folder. \n","\n","1. Please copy the output from the previous code snippet\n","\n","2. Replace the old list from the parameters, \"pick_file\". \n","\n","3. Select whether you want single file or whole folder from the dropdown option on the right.\n","\n","4. If you select single file, please make sure you choose the desired file to extract on the pick_file option as well. If you select whole folder, just ignore the pick_file option.\n","\n","5. After everything is ready, run this code snippet."]},{"cell_type":"code","metadata":{"id":"__JGiEr-FpMK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":425},"executionInfo":{"status":"error","timestamp":1593139784953,"user_tz":-600,"elapsed":22263,"user":{"displayName":"Matthew Leung","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gig2zs-xXencuoCLb7KGW9XqNONCd67o5omaxOP=s64","userId":"15620748170229253400"}},"outputId":"0cd12ca4-73f5-4512-964b-9e0dc2d06987"},"source":["#@markdown Sinlge file or multiple files as input.\n","singleFile_or_multipleFiles = 'single file'  #@param ['single file', 'whole folder'] {allow-input: true}\n","pick_file = \"20060131_!!00NE2G_Prospectus_SD000000000017379366.pdf\" #@param ['20060131_!!00NE2G_Prospectus_SD000000000017379366.pdf', '20070619_!!00XY0D_Prospectus_SD000000000064855320.pdf', '20070705_!!00YPNU_Prospectus_SD000000000065504230.pdf'] {allow-input: true}\n","\n","\n","if singleFile_or_multipleFiles == 'single file':\n","    input_p = root_folder + '/' + pick_file\n","else:\n","    input_p = root_folder\n","\n","# Read pdf files by using class object TableHero\n","# And save it to a folder called 'allPaga_Image'\n","tableHero = TableHero(input_path = input_p)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The input is one pdf file.\n","Transfering to JPG for table detection:\n"," /content/drive/My Drive/capstone/Fianl slides/test files/20060131_!!00NE2G_Prospectus_SD000000000017379366.pdf \n","This may take a while...\n","Convert all PDF files succussfully!\n"],"name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-2d5d1faf25ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Read pdf files by using class object TableHero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# And save it to a folder called 'allPaga_Image'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtableHero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTableHero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-2-f46d6399b515>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_path, pdfNames, jpgFileName, training_data_json, training_data, final_detection_model, predictor, table_train_metadata, crop_table_list)\u001b[0m\n\u001b[1;32m    194\u001b[0m         self.predictor, self.table_train_metadata = load_detection_model(training_data_json,\n\u001b[1;32m    195\u001b[0m                                                                          \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                                                                          final_detection_model)\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdetect_tables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-2-f46d6399b515>\u001b[0m in \u001b[0;36mload_detection_model\u001b[0;34m(training_data_json, training_data, final_detection_model)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROI_HEADS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCORE_THRESH_TEST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m  \u001b[0;31m# set the testing threshold for this model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mpredictor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDefaultPredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mtable_train_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetadataCatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"my_dataset_train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mcheckpointer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDetectionCheckpointer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mcheckpointer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMODEL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWEIGHTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         self.transform_gen = T.ResizeShortestEdge(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/fvcore/common/checkpoint.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, checkpointables)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPathManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Checkpoint {} not found!\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: Checkpoint /content/drive/My Drive/unit of study/capstone/model/model_final_15_450.pth not found!"]}]},{"cell_type":"markdown","metadata":{"id":"ij45s6-cwH11","colab_type":"text"},"source":["### Demo - detect_tables()\n","\n","tableHero.detect_tables() will perform a three step operation here.\n","\n","**Output: 1. Page_with_Bbox folder, 2. Table_JPG, 3. DetectionResult.csv**\n","\n","1. tableHero will first identify all the tables that contains in the pdf documents regardless of it is a target table or non-target table and store it in the folder named \"Page_with_Bbox\". In this folder, it contains all the orignal pdf pages that table in that page will be bound with a bounding box. \n","\n","2. tableHero then will start filtering the result on folder \"Page_with_Bbox\", it starts to filter out those pdf pages with non-target tables since we only want tables that has principal amount and ratings etc.\n","\n","3. tableHero will generate DetectionResult.csv at last. This csv file contains several column, which are PDF file names, page number, table number, page with bounding box and Table Image Name. If column \"Table Image Name\" cell is not empty, then it means that is the required table image and vice versa. \n","\n"]},{"cell_type":"code","metadata":{"id":"gALh29lX7Fxl","colab_type":"code","colab":{}},"source":["tableHero.detect_tables()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gk6hXjhPwYiA","colab_type":"text"},"source":["### Demo - extract_table()\n","\n","**Output: 1. Table_DB.db **\n","\n","extract_table('/content/Table_JPG', 'regular')\n","\n","This function here :\n","\n","1. extract all the image from \"Table_JPG\" folder that we generate from the previous code snippet into a .db file. It uses regular expression to determine whether a row should be identified as header row or not. For details, please go to Major function Snippet \"regular\" section.\n","\n","ps. db file contains all table that can be import into sqlite database and we also create a special table named \"column_summary\" that shows all the headers from different tables per user requirements.\n","\n"]},{"cell_type":"code","metadata":{"id":"MxyEtKQpPlGu","colab_type":"code","colab":{}},"source":[" extract_table('/content/Table_JPG', 'regular')"],"execution_count":null,"outputs":[]}]}